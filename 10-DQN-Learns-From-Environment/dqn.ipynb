{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "EPISODES = 100  # number of episodes\n",
    "EPS_START = 0.9  # e-greedy threshold start value\n",
    "EPS_END = 0.05  # e-greedy threshold end value\n",
    "EPS_DECAY = 200  # e-greedy threshold decay\n",
    "GAMMA = 0.8  # Q-learning discount factor\n",
    "LR = 0.001  # NN optimizer learning rate\n",
    "HIDDEN_LAYER = 256  # NN hidden layer size\n",
    "BATCH_SIZE = 64  # Q-learning batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if gpu is to be used\n",
    "use_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(4, HIDDEN_LAYER),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_LAYER, 2)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "memory = deque(maxlen=10000)\n",
    "optimizer = optim.Adam(model.parameters(), LR)\n",
    "steps_done = 0\n",
    "episode_durations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DQNAgent:\n",
    "#     def __init__(self):\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(4, HIDDEN_LAYER),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(HIDDEN_LAYER, 2)\n",
    "#         )\n",
    "    \n",
    "#     def act(state):\n",
    "#         state = torch.FloatTensor(state)\n",
    "#         eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "#         steps_done += 1\n",
    "#         if np.random.uniform() > eps_threshold:\n",
    "#             return model(Variable(state, volatile=True)).data.max(1)[1].view(1, 1)\n",
    "#         else:\n",
    "#             return torch.LongTensor([[random.randrange(2)]])\n",
    "        \n",
    "#     def memorize():\n",
    "#         pass\n",
    "        \n",
    "#     def learn():\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    global steps_done\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if random.random() > eps_threshold:\n",
    "        return model(Variable(state, volatile=True)).data.max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.LongTensor([[random.randrange(2)]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(e, environment):\n",
    "    state = environment.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        environment.render()\n",
    "        action = select_action(torch.FloatTensor([state]))\n",
    "        next_state, reward, done, _ = environment.step(action.item())\n",
    "\n",
    "        # negative reward when attempt ends\n",
    "        if done:\n",
    "            reward = -1\n",
    "\n",
    "        memory.append((torch.FloatTensor([state]),\n",
    "                      action,  # action is already a tensor\n",
    "                      torch.FloatTensor([next_state]),\n",
    "                      torch.FloatTensor([reward])))\n",
    "\n",
    "        learn()\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "\n",
    "        if done:\n",
    "            print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                  .format(e, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "            episode_durations.append(steps)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    # random transition batch is taken from experience replay memory\n",
    "    transitions = random.sample(memory, BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_next_state, batch_reward = zip(*transitions)\n",
    "\n",
    "    batch_state = torch.cat(batch_state)\n",
    "    batch_action = torch.cat(batch_action)\n",
    "    batch_reward = torch.cat(batch_reward)\n",
    "    batch_next_state = torch.cat(batch_next_state)\n",
    "\n",
    "    # current Q values are estimated by NN for all actions\n",
    "    current_q_values = model(batch_state).gather(1, batch_action)\n",
    "    # expected Q values are estimated from actions which gives maximum Q value\n",
    "    max_next_q_values = model(batch_next_state).detach().max(1)[0]\n",
    "    expected_q_values = batch_reward + (GAMMA * max_next_q_values)\n",
    "\n",
    "    # loss is measured from error between current and newly expected Q values\n",
    "    loss = F.smooth_l1_loss(current_q_values.squeeze(), expected_q_values)\n",
    "\n",
    "    # backpropagation of loss to NN\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[99m Episode 0 finished after 37 steps\n",
      "\u001b[99m Episode 1 finished after 30 steps\n",
      "\u001b[99m Episode 2 finished after 13 steps\n",
      "\u001b[99m Episode 3 finished after 11 steps\n",
      "\u001b[99m Episode 4 finished after 13 steps\n",
      "\u001b[99m Episode 5 finished after 13 steps\n",
      "\u001b[99m Episode 6 finished after 11 steps\n",
      "\u001b[99m Episode 7 finished after 13 steps\n",
      "\u001b[99m Episode 8 finished after 11 steps\n",
      "\u001b[99m Episode 9 finished after 10 steps\n",
      "\u001b[99m Episode 10 finished after 11 steps\n",
      "\u001b[99m Episode 11 finished after 9 steps\n",
      "\u001b[99m Episode 12 finished after 10 steps\n",
      "\u001b[99m Episode 13 finished after 12 steps\n",
      "\u001b[99m Episode 14 finished after 10 steps\n",
      "\u001b[99m Episode 15 finished after 10 steps\n",
      "\u001b[99m Episode 16 finished after 12 steps\n",
      "\u001b[99m Episode 17 finished after 8 steps\n",
      "\u001b[99m Episode 18 finished after 10 steps\n",
      "\u001b[99m Episode 19 finished after 10 steps\n",
      "\u001b[99m Episode 20 finished after 11 steps\n",
      "\u001b[99m Episode 21 finished after 9 steps\n",
      "\u001b[99m Episode 22 finished after 11 steps\n",
      "\u001b[99m Episode 23 finished after 9 steps\n",
      "\u001b[99m Episode 24 finished after 9 steps\n",
      "\u001b[99m Episode 25 finished after 12 steps\n",
      "\u001b[99m Episode 26 finished after 13 steps\n",
      "\u001b[99m Episode 27 finished after 11 steps\n",
      "\u001b[99m Episode 28 finished after 14 steps\n",
      "\u001b[99m Episode 29 finished after 14 steps\n",
      "\u001b[99m Episode 30 finished after 17 steps\n",
      "\u001b[99m Episode 31 finished after 11 steps\n",
      "\u001b[99m Episode 32 finished after 28 steps\n",
      "\u001b[99m Episode 33 finished after 26 steps\n",
      "\u001b[99m Episode 34 finished after 20 steps\n",
      "\u001b[99m Episode 35 finished after 28 steps\n",
      "\u001b[99m Episode 36 finished after 29 steps\n",
      "\u001b[99m Episode 37 finished after 25 steps\n",
      "\u001b[99m Episode 38 finished after 63 steps\n",
      "\u001b[99m Episode 39 finished after 43 steps\n",
      "\u001b[99m Episode 40 finished after 93 steps\n",
      "\u001b[99m Episode 41 finished after 95 steps\n",
      "\u001b[99m Episode 42 finished after 54 steps\n",
      "\u001b[99m Episode 43 finished after 72 steps\n",
      "\u001b[99m Episode 44 finished after 68 steps\n",
      "\u001b[99m Episode 45 finished after 132 steps\n",
      "\u001b[99m Episode 46 finished after 134 steps\n",
      "\u001b[99m Episode 47 finished after 155 steps\n",
      "\u001b[99m Episode 48 finished after 132 steps\n",
      "\u001b[99m Episode 49 finished after 117 steps\n",
      "\u001b[92m Episode 50 finished after 200 steps\n",
      "\u001b[99m Episode 51 finished after 123 steps\n",
      "\u001b[99m Episode 52 finished after 157 steps\n",
      "\u001b[99m Episode 53 finished after 122 steps\n",
      "\u001b[99m Episode 54 finished after 129 steps\n",
      "\u001b[92m Episode 55 finished after 200 steps\n",
      "\u001b[92m Episode 56 finished after 200 steps\n",
      "\u001b[99m Episode 57 finished after 130 steps\n",
      "\u001b[99m Episode 58 finished after 172 steps\n",
      "\u001b[92m Episode 59 finished after 200 steps\n",
      "\u001b[92m Episode 60 finished after 200 steps\n",
      "\u001b[92m Episode 61 finished after 200 steps\n",
      "\u001b[92m Episode 62 finished after 200 steps\n",
      "\u001b[92m Episode 63 finished after 200 steps\n",
      "\u001b[92m Episode 64 finished after 200 steps\n",
      "\u001b[92m Episode 65 finished after 200 steps\n",
      "\u001b[92m Episode 66 finished after 200 steps\n",
      "\u001b[92m Episode 67 finished after 200 steps\n",
      "\u001b[92m Episode 68 finished after 197 steps\n",
      "\u001b[99m Episode 69 finished after 164 steps\n",
      "\u001b[92m Episode 70 finished after 200 steps\n",
      "\u001b[92m Episode 71 finished after 200 steps\n",
      "\u001b[92m Episode 72 finished after 200 steps\n",
      "\u001b[92m Episode 73 finished after 200 steps\n",
      "\u001b[99m Episode 74 finished after 186 steps\n",
      "\u001b[92m Episode 75 finished after 200 steps\n",
      "\u001b[92m Episode 76 finished after 200 steps\n",
      "\u001b[99m Episode 77 finished after 169 steps\n",
      "\u001b[92m Episode 78 finished after 200 steps\n",
      "\u001b[92m Episode 79 finished after 200 steps\n",
      "\u001b[92m Episode 80 finished after 200 steps\n",
      "\u001b[99m Episode 81 finished after 174 steps\n",
      "\u001b[99m Episode 82 finished after 184 steps\n",
      "\u001b[92m Episode 83 finished after 200 steps\n",
      "\u001b[92m Episode 84 finished after 200 steps\n",
      "\u001b[92m Episode 85 finished after 200 steps\n",
      "\u001b[92m Episode 86 finished after 200 steps\n",
      "\u001b[92m Episode 87 finished after 200 steps\n",
      "\u001b[99m Episode 88 finished after 191 steps\n",
      "\u001b[99m Episode 89 finished after 177 steps\n",
      "\u001b[92m Episode 90 finished after 200 steps\n",
      "\u001b[99m Episode 91 finished after 187 steps\n",
      "\u001b[92m Episode 92 finished after 200 steps\n",
      "\u001b[99m Episode 93 finished after 176 steps\n",
      "\u001b[99m Episode 94 finished after 194 steps\n",
      "\u001b[92m Episode 95 finished after 200 steps\n",
      "\u001b[99m Episode 96 finished after 170 steps\n",
      "\u001b[99m Episode 97 finished after 187 steps\n",
      "\u001b[92m Episode 98 finished after 200 steps\n",
      "\u001b[92m Episode 99 finished after 200 steps\n"
     ]
    }
   ],
   "source": [
    "for e in range(EPISODES):\n",
    "    run_episode(e, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
