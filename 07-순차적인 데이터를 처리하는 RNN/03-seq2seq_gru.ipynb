{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello ->  [104, 101, 108, 108, 111]\n",
      "hola  ->  [104, 111, 108, 97]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 256  # ascii size\n",
    "x_ = list(map(ord, \"hello\"))  # convert to list of ascii codes\n",
    "y_ = list(map(ord, \"hola\"))   # convert to list of ascii codes\n",
    "print(\"hello -> \", x_)\n",
    "print(\"hola  -> \", y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(th.LongTensor(x_))\n",
    "y = Variable(th.LongTensor(y_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([104, 101, 108, 108, 111])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model using GRU and conventional concatenating motion.\n",
    "'''\n",
    "class Seq2Seq_GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(Seq2Seq_GRU, self).__init__()\n",
    "\n",
    "        self.n_layers = 1\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.encoder = nn.GRU(hidden_size, hidden_size)\n",
    "        self.decoder = nn.GRU(hidden_size * 2, hidden_size)\n",
    "        self.project = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Encoder inputs and states\n",
    "        initial_state = self._init_state()\n",
    "        embedding = self.embedding(inputs).unsqueeze(1)\n",
    "        encoder_output, encoder_state = self.encoder(embedding, initial_state)\n",
    "        outputs = []\n",
    "\n",
    "        decoder_state = encoder_state\n",
    "        for i in range(targets.size()[0]): \n",
    "            decoder_input = self.embedding(targets)[i].view(1,-1, self.hidden_size)\n",
    "            decoder_input = th.cat((decoder_input, encoder_state), 2)\n",
    "            decoder_output, decoder_state = self.decoder(decoder_input, decoder_state)\n",
    "            projection = self.project(decoder_output)#.unsqueeze(0))\n",
    "            outputs.append(projection)\n",
    "            \n",
    "            #_, top_i = prediction.data.topk(1)\n",
    "            \n",
    "        outputs = th.stack(outputs, 1).squeeze()\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def _init_state(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(self.n_layers, batch_size, self.hidden_size).zero_()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq_GRU(vocab_size, 16)\n",
    "pred = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_.append(3)\n",
    "y_label = Variable(th.LongTensor(y_[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([111, 108,  97,   3])\n"
     ]
    }
   ],
   "source": [
    "print(y_label.shape)\n",
    "print(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 5.611233234405518\n",
      "E 7 V 4 \n",
      "100 loss: 1.9432737827301025\n",
      "h o l a \n",
      "200 loss: 0.5616824626922607\n",
      "h o l a \n",
      "300 loss: 0.2673376798629761\n",
      "h o l a \n",
      "400 loss: 0.16206204891204834\n",
      "h o l a \n",
      "500 loss: 0.11266088485717773\n",
      "h o l a \n",
      "600 loss: 0.08434617519378662\n",
      "h o l a \n",
      "700 loss: 0.06614077091217041\n",
      "h o l a \n",
      "800 loss: 0.05353283882141113\n",
      "h o l a \n",
      "900 loss: 0.04404950141906738\n",
      "h o l a \n",
      "1000 loss: 0.03680562973022461\n",
      "h o l a \n",
      "1100 loss: 0.03102242946624756\n",
      "h o l a \n",
      "1200 loss: 0.026388049125671387\n",
      "h o l a \n",
      "1300 loss: 0.022706985473632812\n",
      "h o l a \n",
      "1400 loss: 0.019757390022277832\n",
      "h o l a \n",
      "1500 loss: 0.0173567533493042\n",
      "h o l a \n",
      "1600 loss: 0.015371441841125488\n",
      "h o l a \n",
      "1700 loss: 0.013707399368286133\n",
      "h o l a \n",
      "1800 loss: 0.012296199798583984\n",
      "h o l a \n",
      "1900 loss: 0.011086702346801758\n",
      "h o l a \n",
      "2000 loss: 0.010039806365966797\n",
      "h o l a \n",
      "2100 loss: 0.009126663208007812\n",
      "h o l a \n",
      "2200 loss: 0.008322954177856445\n",
      "h o l a \n",
      "2300 loss: 0.007612466812133789\n",
      "h o l a \n",
      "2400 loss: 0.006980180740356445\n",
      "h o l a \n",
      "2500 loss: 0.00641632080078125\n",
      "h o l a \n",
      "2600 loss: 0.005910158157348633\n",
      "h o l a \n",
      "2700 loss: 0.005454301834106445\n",
      "h o l a \n",
      "2800 loss: 0.0050411224365234375\n",
      "h o l a \n",
      "2900 loss: 0.004665851593017578\n",
      "h o l a \n",
      "3000 loss: 0.004323482513427734\n",
      "h o l a \n",
      "3100 loss: 0.004010438919067383\n",
      "h o l a \n",
      "3200 loss: 0.00372314453125\n",
      "h o l a \n",
      "3300 loss: 0.0034592151641845703\n",
      "h o l a \n",
      "3400 loss: 0.0032155513763427734\n",
      "h o l a \n",
      "3500 loss: 0.0029909610748291016\n",
      "h o l a \n",
      "3600 loss: 0.0027837753295898438\n",
      "h o l a \n",
      "3700 loss: 0.0025925636291503906\n",
      "h o l a \n",
      "3800 loss: 0.0024156570434570312\n",
      "h o l a \n",
      "3900 loss: 0.002252817153930664\n",
      "h o l a \n",
      "4000 loss: 0.002102375030517578\n",
      "h o l a \n",
      "4100 loss: 0.001963376998901367\n",
      "h o l a \n",
      "4200 loss: 0.0018358230590820312\n",
      "h o l a \n",
      "4300 loss: 0.0017178058624267578\n",
      "h o l a \n",
      "4400 loss: 0.0016088485717773438\n",
      "h o l a \n",
      "4500 loss: 0.0015087127685546875\n",
      "h o l a \n",
      "4600 loss: 0.0014154911041259766\n",
      "h o l a \n",
      "4700 loss: 0.0013294219970703125\n",
      "h o l a \n",
      "4800 loss: 0.001249551773071289\n",
      "h o l a \n",
      "4900 loss: 0.0011754035949707031\n",
      "h o l a \n",
      "5000 loss: 0.00110626220703125\n",
      "h o l a \n",
      "5100 loss: 0.0010418891906738281\n",
      "h o l a \n",
      "5200 loss: 0.000982046127319336\n",
      "h o l a \n",
      "5300 loss: 0.0009260177612304688\n",
      "h o l a \n",
      "5400 loss: 0.0008738040924072266\n",
      "h o l a \n",
      "5500 loss: 0.0008246898651123047\n",
      "h o l a \n",
      "5600 loss: 0.0007789134979248047\n",
      "h o l a \n",
      "5700 loss: 0.000736236572265625\n",
      "h o l a \n",
      "5800 loss: 0.0006957054138183594\n",
      "h o l a \n",
      "5900 loss: 0.0006580352783203125\n",
      "h o l a \n",
      "6000 loss: 0.0006222724914550781\n",
      "h o l a \n",
      "6100 loss: 0.0005888938903808594\n",
      "h o l a \n",
      "6200 loss: 0.0005574226379394531\n",
      "h o l a \n",
      "6300 loss: 0.0005280971527099609\n",
      "h o l a \n",
      "6400 loss: 0.000499725341796875\n",
      "h o l a \n",
      "6500 loss: 0.0004734992980957031\n",
      "h o l a \n",
      "6600 loss: 0.0004489421844482422\n",
      "h o l a \n",
      "6700 loss: 0.00042557716369628906\n",
      "h o l a \n",
      "6800 loss: 0.0004029273986816406\n",
      "h o l a \n",
      "6900 loss: 0.0003821849822998047\n",
      "h o l a \n",
      "7000 loss: 0.0003619194030761719\n",
      "h o l a \n",
      "7100 loss: 0.0003426074981689453\n",
      "h o l a \n",
      "7200 loss: 0.000324249267578125\n",
      "h o l a \n",
      "7300 loss: 0.00030612945556640625\n",
      "h o l a \n",
      "7400 loss: 0.0002872943878173828\n",
      "h o l a \n",
      "7500 loss: 0.0002694129943847656\n",
      "h o l a \n",
      "7600 loss: 0.0002536773681640625\n",
      "h o l a \n",
      "7700 loss: 0.00023937225341796875\n",
      "h o l a \n",
      "7800 loss: 0.00022673606872558594\n",
      "h o l a \n",
      "7900 loss: 0.0002143383026123047\n",
      "h o l a \n",
      "8000 loss: 0.0002028942108154297\n",
      "h o l a \n",
      "8100 loss: 0.00019216537475585938\n",
      "h o l a \n",
      "8200 loss: 0.0001819133758544922\n",
      "h o l a \n",
      "8300 loss: 0.00017189979553222656\n",
      "h o l a \n",
      "8400 loss: 0.0001628398895263672\n",
      "h o l a \n",
      "8500 loss: 0.00015401840209960938\n",
      "h o l a \n",
      "8600 loss: 0.00014591217041015625\n",
      "h o l a \n",
      "8700 loss: 0.0001380443572998047\n",
      "h o l a \n",
      "8800 loss: 0.0001308917999267578\n",
      "h o l a \n",
      "8900 loss: 0.00012350082397460938\n",
      "h o l a \n",
      "9000 loss: 0.00011730194091796875\n",
      "h o l a \n",
      "9100 loss: 0.00011134147644042969\n",
      "h o l a \n",
      "9200 loss: 0.00010514259338378906\n",
      "h o l a \n",
      "9300 loss: 9.965896606445312e-05\n",
      "h o l a \n",
      "9400 loss: 9.441375732421875e-05\n",
      "h o l a \n",
      "9500 loss: 8.940696716308594e-05\n",
      "h o l a \n",
      "9600 loss: 8.487701416015625e-05\n",
      "h o l a \n",
      "9700 loss: 8.034706115722656e-05\n",
      "h o l a \n",
      "9800 loss: 7.605552673339844e-05\n",
      "h o l a \n",
      "9900 loss: 7.224082946777344e-05\n",
      "h o l a \n"
     ]
    }
   ],
   "source": [
    "log = []\n",
    "for i in range(10000):\n",
    "    prediction = model(x, y)\n",
    "    loss = criterion(prediction, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_val = loss.data\n",
    "    log.append(loss_val)\n",
    "    if i % 100 == 0:\n",
    "        print(\"%d loss: %s\" % (i, loss_val.item()))\n",
    "        _, top1 = prediction.data.topk(1, 1)\n",
    "        for c in top1.squeeze().numpy().tolist():\n",
    "            print(chr(c), end=\" \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(log)\n",
    "plt.xlim(0,150)\n",
    "plt.ylim(0,15)\n",
    "plt.ylabel('cross entropy loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
