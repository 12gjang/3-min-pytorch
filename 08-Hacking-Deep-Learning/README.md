# 딥러닝 해킹하기

* [개념] Adversarial Attack 이란?
* [프로젝트 1] FGSM 공격

신기하게도 사람이 착시를 보듯 딥러닝 모델도 일종의 '착시'를 봅니다. 
다만 컴퓨터가 보는 착시는 사람의 그것과 다른 종류입니다.
머신러닝 모델의 실수를 유도하는 입력을 적대적 예제(Adversarial Example)라고 합니다.
의도적으로 적대적 예제를 생성해서
머신러닝 기반 시스템에 보안 문제를 야기하는
적대적 공격(Adversarial Attack)에 대해서 자세히 알아보겠습니다.

머신러닝에 의존한 서비스와 시스템이 많아지면서 