{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 신경망 깊게 쌓아 컬러 데이터셋에 적용하기\n",
    "Convolutional Neural Network (CNN) 을 쌓아올려 딥한 러닝을 해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS     = 40\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5),\n",
    "                                                     (0.5, 0.5, 0.5))])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./.data',\n",
    "                   train=True,\n",
    "                   download=True,\n",
    "                   transform=transform),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.CIFAR10('./.data',\n",
    "                   train=False, \n",
    "                   transform=transform),\n",
    "    batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 뉴럴넷으로 Fashion MNIST 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64,  2, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, 2, stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 \n",
    "\n",
    "`to()` 함수는 모델의 파라미터들을 지정한 곳으로 보내는 역할을 합니다. 일반적으로 CPU 1개만 사용할 경우 필요는 없지만, GPU를 사용하고자 하는 경우 `to(\"cuda\")`로 지정하여 GPU로 보내야 합니다. 지정하지 않을 경우 계속 CPU에 남아 있게 되며 빠른 훈련의 이점을 누리실 수 없습니다.\n",
    "\n",
    "최적화 알고리즘으로 파이토치에 내장되어 있는 `optim.SGD`를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = ResNet18().to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 테스트하기\n",
    "\n",
    "아무리 훈련이 잘 되었다고 해도 실제 데이터를 만났을때 성능이 낮다면 쓸모 없는 모델일 것입니다. 우리가 진정 원하는 것은 훈련 데이터에 최적화한 모델이 아니라 모든 데이터에서 높은 성능을 보이는 모델이기 때문입니다. 세상에 존재하는 모든 데이터에 최적화 하는 것을 \"일반화\"라고 부르고 모델이 얼마나 실제 데이터에 적응하는지를 수치로 나타낸 것을 \"일반화 오류\"(Generalization Error) 라고 합니다. \n",
    "\n",
    "우리가 만든 모델이 얼마나 일반화를 잘 하는지 알아보기 위해, 그리고 언제 훈련을 멈추어야 할지 알기 위해 매 이포크가 끝날때 마다 테스트셋으로 모델의 성능을 측정해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            # sum up batch loss\n",
    "            test_loss += F.cross_entropy(output, target,\n",
    "                                         size_average=False).item()\n",
    "\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 돌려보기\n",
    "\n",
    "자, 이제 모든 준비가 끝났습니다. 코드를 돌려서 실제로 훈련이 되는지 확인해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.346486\n",
      "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.781493\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.462888\n",
      "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.427824\n",
      "[1] Test Loss: 1.2828, Accuracy: 54.95%\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.175778\n",
      "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.431858\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 0.757462\n",
      "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 0.960510\n",
      "[2] Test Loss: 1.0339, Accuracy: 64.79%\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 0.896164\n",
      "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 0.528886\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 0.431045\n",
      "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 0.592155\n",
      "[3] Test Loss: 0.8495, Accuracy: 71.06%\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.654060\n",
      "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 0.657335\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 0.601032\n",
      "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 0.566090\n",
      "[4] Test Loss: 0.7315, Accuracy: 75.64%\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 0.263753\n",
      "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 0.593298\n",
      "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 0.482006\n",
      "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 0.395781\n",
      "[5] Test Loss: 0.6206, Accuracy: 79.55%\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 0.288268\n",
      "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 0.175379\n",
      "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 0.274902\n",
      "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 0.329684\n",
      "[6] Test Loss: 0.6409, Accuracy: 79.73%\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.298168\n",
      "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.117776\n",
      "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 0.201086\n",
      "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.301488\n",
      "[7] Test Loss: 0.5961, Accuracy: 82.28%\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 0.074480\n",
      "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 0.021090\n",
      "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.149082\n",
      "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.121252\n",
      "[8] Test Loss: 0.6771, Accuracy: 81.77%\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 0.092181\n",
      "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.169552\n",
      "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 0.237744\n",
      "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 0.205062\n",
      "[9] Test Loss: 0.8272, Accuracy: 81.38%\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 0.090124\n",
      "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.066874\n",
      "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.095450\n",
      "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.080977\n",
      "[10] Test Loss: 0.7713, Accuracy: 81.86%\n",
      "Train Epoch: 11 [0/50000 (0%)]\tLoss: 0.042156\n",
      "Train Epoch: 11 [12800/50000 (26%)]\tLoss: 0.133412\n",
      "Train Epoch: 11 [25600/50000 (51%)]\tLoss: 0.054302\n",
      "Train Epoch: 11 [38400/50000 (77%)]\tLoss: 0.121198\n",
      "[11] Test Loss: 0.9508, Accuracy: 79.58%\n",
      "Train Epoch: 12 [0/50000 (0%)]\tLoss: 0.019359\n",
      "Train Epoch: 12 [12800/50000 (26%)]\tLoss: 0.040471\n",
      "Train Epoch: 12 [25600/50000 (51%)]\tLoss: 0.078572\n",
      "Train Epoch: 12 [38400/50000 (77%)]\tLoss: 0.017300\n",
      "[12] Test Loss: 0.8915, Accuracy: 80.59%\n",
      "Train Epoch: 13 [0/50000 (0%)]\tLoss: 0.039379\n",
      "Train Epoch: 13 [12800/50000 (26%)]\tLoss: 0.051432\n",
      "Train Epoch: 13 [25600/50000 (51%)]\tLoss: 0.042938\n",
      "Train Epoch: 13 [38400/50000 (77%)]\tLoss: 0.086489\n",
      "[13] Test Loss: 0.9751, Accuracy: 80.28%\n",
      "Train Epoch: 14 [0/50000 (0%)]\tLoss: 0.036708\n",
      "Train Epoch: 14 [12800/50000 (26%)]\tLoss: 0.038054\n",
      "Train Epoch: 14 [25600/50000 (51%)]\tLoss: 0.056710\n",
      "Train Epoch: 14 [38400/50000 (77%)]\tLoss: 0.099445\n",
      "[14] Test Loss: 0.9005, Accuracy: 81.80%\n",
      "Train Epoch: 15 [0/50000 (0%)]\tLoss: 0.075209\n",
      "Train Epoch: 15 [12800/50000 (26%)]\tLoss: 0.020349\n",
      "Train Epoch: 15 [25600/50000 (51%)]\tLoss: 0.006081\n",
      "Train Epoch: 15 [38400/50000 (77%)]\tLoss: 0.108804\n",
      "[15] Test Loss: 0.8963, Accuracy: 81.92%\n",
      "Train Epoch: 16 [0/50000 (0%)]\tLoss: 0.003105\n",
      "Train Epoch: 16 [12800/50000 (26%)]\tLoss: 0.013763\n",
      "Train Epoch: 16 [25600/50000 (51%)]\tLoss: 0.144654\n",
      "Train Epoch: 16 [38400/50000 (77%)]\tLoss: 0.019058\n",
      "[16] Test Loss: 1.0124, Accuracy: 80.88%\n",
      "Train Epoch: 17 [0/50000 (0%)]\tLoss: 0.022359\n",
      "Train Epoch: 17 [12800/50000 (26%)]\tLoss: 0.017958\n",
      "Train Epoch: 17 [25600/50000 (51%)]\tLoss: 0.008235\n",
      "Train Epoch: 17 [38400/50000 (77%)]\tLoss: 0.045691\n",
      "[17] Test Loss: 0.9895, Accuracy: 81.21%\n",
      "Train Epoch: 18 [0/50000 (0%)]\tLoss: 0.077323\n",
      "Train Epoch: 18 [12800/50000 (26%)]\tLoss: 0.046082\n",
      "Train Epoch: 18 [25600/50000 (51%)]\tLoss: 0.015074\n",
      "Train Epoch: 18 [38400/50000 (77%)]\tLoss: 0.080469\n",
      "[18] Test Loss: 0.9890, Accuracy: 82.15%\n",
      "Train Epoch: 19 [0/50000 (0%)]\tLoss: 0.077440\n",
      "Train Epoch: 19 [12800/50000 (26%)]\tLoss: 0.014446\n",
      "Train Epoch: 19 [25600/50000 (51%)]\tLoss: 0.022698\n",
      "Train Epoch: 19 [38400/50000 (77%)]\tLoss: 0.179370\n",
      "[19] Test Loss: 0.9353, Accuracy: 81.79%\n",
      "Train Epoch: 20 [0/50000 (0%)]\tLoss: 0.041024\n",
      "Train Epoch: 20 [12800/50000 (26%)]\tLoss: 0.006192\n",
      "Train Epoch: 20 [25600/50000 (51%)]\tLoss: 0.054234\n",
      "Train Epoch: 20 [38400/50000 (77%)]\tLoss: 0.031837\n",
      "[20] Test Loss: 1.0373, Accuracy: 81.63%\n",
      "Train Epoch: 21 [0/50000 (0%)]\tLoss: 0.002257\n",
      "Train Epoch: 21 [12800/50000 (26%)]\tLoss: 0.036443\n",
      "Train Epoch: 21 [25600/50000 (51%)]\tLoss: 0.004359\n",
      "Train Epoch: 21 [38400/50000 (77%)]\tLoss: 0.025907\n",
      "[21] Test Loss: 1.1093, Accuracy: 79.89%\n",
      "Train Epoch: 22 [0/50000 (0%)]\tLoss: 0.047059\n",
      "Train Epoch: 22 [12800/50000 (26%)]\tLoss: 0.032460\n",
      "Train Epoch: 22 [25600/50000 (51%)]\tLoss: 0.021577\n",
      "Train Epoch: 22 [38400/50000 (77%)]\tLoss: 0.012811\n",
      "[22] Test Loss: 1.0219, Accuracy: 81.70%\n",
      "Train Epoch: 23 [0/50000 (0%)]\tLoss: 0.045547\n",
      "Train Epoch: 23 [12800/50000 (26%)]\tLoss: 0.030235\n",
      "Train Epoch: 23 [25600/50000 (51%)]\tLoss: 0.008075\n",
      "Train Epoch: 23 [38400/50000 (77%)]\tLoss: 0.037290\n",
      "[23] Test Loss: 1.0650, Accuracy: 81.26%\n",
      "Train Epoch: 24 [0/50000 (0%)]\tLoss: 0.010475\n",
      "Train Epoch: 24 [12800/50000 (26%)]\tLoss: 0.003203\n",
      "Train Epoch: 24 [25600/50000 (51%)]\tLoss: 0.042605\n",
      "Train Epoch: 24 [38400/50000 (77%)]\tLoss: 0.023618\n",
      "[24] Test Loss: 1.1046, Accuracy: 81.91%\n",
      "Train Epoch: 25 [0/50000 (0%)]\tLoss: 0.002533\n",
      "Train Epoch: 25 [12800/50000 (26%)]\tLoss: 0.073737\n",
      "Train Epoch: 25 [25600/50000 (51%)]\tLoss: 0.000422\n",
      "Train Epoch: 25 [38400/50000 (77%)]\tLoss: 0.062355\n",
      "[25] Test Loss: 1.1137, Accuracy: 80.76%\n",
      "Train Epoch: 26 [0/50000 (0%)]\tLoss: 0.043246\n",
      "Train Epoch: 26 [12800/50000 (26%)]\tLoss: 0.009686\n",
      "Train Epoch: 26 [25600/50000 (51%)]\tLoss: 0.062698\n",
      "Train Epoch: 26 [38400/50000 (77%)]\tLoss: 0.041360\n",
      "[26] Test Loss: 1.0121, Accuracy: 82.76%\n",
      "Train Epoch: 27 [0/50000 (0%)]\tLoss: 0.002314\n",
      "Train Epoch: 27 [12800/50000 (26%)]\tLoss: 0.027061\n",
      "Train Epoch: 27 [25600/50000 (51%)]\tLoss: 0.014165\n",
      "Train Epoch: 27 [38400/50000 (77%)]\tLoss: 0.054768\n",
      "[27] Test Loss: 1.1063, Accuracy: 81.93%\n",
      "Train Epoch: 28 [0/50000 (0%)]\tLoss: 0.016691\n",
      "Train Epoch: 28 [12800/50000 (26%)]\tLoss: 0.001740\n",
      "Train Epoch: 28 [25600/50000 (51%)]\tLoss: 0.037564\n",
      "Train Epoch: 28 [38400/50000 (77%)]\tLoss: 0.015132\n",
      "[28] Test Loss: 1.0995, Accuracy: 81.20%\n",
      "Train Epoch: 29 [0/50000 (0%)]\tLoss: 0.002401\n",
      "Train Epoch: 29 [12800/50000 (26%)]\tLoss: 0.013433\n",
      "Train Epoch: 29 [25600/50000 (51%)]\tLoss: 0.007691\n",
      "Train Epoch: 29 [38400/50000 (77%)]\tLoss: 0.065315\n",
      "[29] Test Loss: 1.0667, Accuracy: 82.34%\n",
      "Train Epoch: 30 [0/50000 (0%)]\tLoss: 0.008746\n",
      "Train Epoch: 30 [12800/50000 (26%)]\tLoss: 0.007522\n",
      "Train Epoch: 30 [25600/50000 (51%)]\tLoss: 0.100617\n",
      "Train Epoch: 30 [38400/50000 (77%)]\tLoss: 0.007215\n",
      "[30] Test Loss: 1.2008, Accuracy: 80.61%\n",
      "Train Epoch: 31 [0/50000 (0%)]\tLoss: 0.029422\n",
      "Train Epoch: 31 [12800/50000 (26%)]\tLoss: 0.078116\n",
      "Train Epoch: 31 [25600/50000 (51%)]\tLoss: 0.004526\n",
      "Train Epoch: 31 [38400/50000 (77%)]\tLoss: 0.005610\n",
      "[31] Test Loss: 1.0634, Accuracy: 82.29%\n",
      "Train Epoch: 32 [0/50000 (0%)]\tLoss: 0.001628\n",
      "Train Epoch: 32 [12800/50000 (26%)]\tLoss: 0.017018\n",
      "Train Epoch: 32 [25600/50000 (51%)]\tLoss: 0.025959\n",
      "Train Epoch: 32 [38400/50000 (77%)]\tLoss: 0.008390\n",
      "[32] Test Loss: 1.0964, Accuracy: 82.23%\n",
      "Train Epoch: 33 [0/50000 (0%)]\tLoss: 0.009299\n",
      "Train Epoch: 33 [12800/50000 (26%)]\tLoss: 0.004417\n",
      "Train Epoch: 33 [25600/50000 (51%)]\tLoss: 0.028298\n",
      "Train Epoch: 33 [38400/50000 (77%)]\tLoss: 0.012650\n",
      "[33] Test Loss: 1.1186, Accuracy: 82.62%\n",
      "Train Epoch: 34 [0/50000 (0%)]\tLoss: 0.002011\n",
      "Train Epoch: 34 [12800/50000 (26%)]\tLoss: 0.001922\n",
      "Train Epoch: 34 [25600/50000 (51%)]\tLoss: 0.115394\n",
      "Train Epoch: 34 [38400/50000 (77%)]\tLoss: 0.001191\n",
      "[34] Test Loss: 1.1568, Accuracy: 81.66%\n",
      "Train Epoch: 35 [0/50000 (0%)]\tLoss: 0.000678\n",
      "Train Epoch: 35 [12800/50000 (26%)]\tLoss: 0.002945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 35 [25600/50000 (51%)]\tLoss: 0.043934\n",
      "Train Epoch: 35 [38400/50000 (77%)]\tLoss: 0.014713\n",
      "[35] Test Loss: 1.1658, Accuracy: 81.52%\n",
      "Train Epoch: 36 [0/50000 (0%)]\tLoss: 0.017344\n",
      "Train Epoch: 36 [12800/50000 (26%)]\tLoss: 0.000506\n",
      "Train Epoch: 36 [25600/50000 (51%)]\tLoss: 0.002387\n",
      "Train Epoch: 36 [38400/50000 (77%)]\tLoss: 0.003844\n",
      "[36] Test Loss: 1.1801, Accuracy: 82.19%\n",
      "Train Epoch: 37 [0/50000 (0%)]\tLoss: 0.001715\n",
      "Train Epoch: 37 [12800/50000 (26%)]\tLoss: 0.011111\n",
      "Train Epoch: 37 [25600/50000 (51%)]\tLoss: 0.004604\n",
      "Train Epoch: 37 [38400/50000 (77%)]\tLoss: 0.065278\n",
      "[37] Test Loss: 1.1691, Accuracy: 82.57%\n",
      "Train Epoch: 38 [0/50000 (0%)]\tLoss: 0.012695\n",
      "Train Epoch: 38 [12800/50000 (26%)]\tLoss: 0.002853\n",
      "Train Epoch: 38 [25600/50000 (51%)]\tLoss: 0.056870\n",
      "Train Epoch: 38 [38400/50000 (77%)]\tLoss: 0.000179\n",
      "[38] Test Loss: 1.2270, Accuracy: 82.09%\n",
      "Train Epoch: 39 [0/50000 (0%)]\tLoss: 0.029769\n",
      "Train Epoch: 39 [12800/50000 (26%)]\tLoss: 0.113176\n",
      "Train Epoch: 39 [25600/50000 (51%)]\tLoss: 0.006993\n",
      "Train Epoch: 39 [38400/50000 (77%)]\tLoss: 0.000655\n",
      "[39] Test Loss: 1.2449, Accuracy: 81.96%\n",
      "Train Epoch: 40 [0/50000 (0%)]\tLoss: 0.004213\n",
      "Train Epoch: 40 [12800/50000 (26%)]\tLoss: 0.059547\n",
      "Train Epoch: 40 [25600/50000 (51%)]\tLoss: 0.017369\n",
      "Train Epoch: 40 [38400/50000 (77%)]\tLoss: 0.154756\n",
      "[40] Test Loss: 1.2058, Accuracy: 82.41%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test_loss, test_accuracy = test(model, test_loader)\n",
    "    \n",
    "    print('[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'.format(\n",
    "          epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
